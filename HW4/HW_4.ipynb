{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMvn0yI9BVz0OurGopKHICk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yektaKamane/GPU_Programming_Course/blob/main/HW4/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kC4_bhA7dib"
      },
      "source": [
        "# HW4 - class\n",
        "The implementation of min and max using reduce algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRzdacXPApqg"
      },
      "source": [
        "## Original version\n",
        "The code below I found in the internet and I'm using it as a starting pint to write my own code.\n",
        "[src link](https://github.com/MaxKotlan/Cuda-Find-Max-Using-Parallel-Reduction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U03yzW-q7y_a",
        "outputId": "fd13745d-e401-4d02-ed7a-86fcbccd4fb6"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define MAX_CUDA_THREADS_PER_BLOCK 1024\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess) \n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "\n",
        "struct Startup{\n",
        "    int random_range = INT_MAX;\n",
        "    int threads_per_block = MAX_CUDA_THREADS_PER_BLOCK;\n",
        "} startup;\n",
        "\n",
        "struct DataSet{\n",
        "    float* values;\n",
        "    int  size;\n",
        "};\n",
        "\n",
        "struct Result{\n",
        "    float MaxValue;\n",
        "    float KernelExecutionTime;\n",
        "};\n",
        "\n",
        "DataSet generateRandomDataSet(int size){\n",
        "    DataSet data;\n",
        "    data.size = size;\n",
        "    data.values = (float*)malloc(sizeof(float)*data.size);\n",
        "\n",
        "    for (int i = 0; i < data.size; i++)\n",
        "        data.values[i] = (float)(rand()%startup.random_range);\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "__global__ void Max_Interleaved_Addressing_Global(float* data, int data_size){\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (idx < data_size){\n",
        "        for(int stride=1; stride < data_size; stride *= 2) {\n",
        "            if (idx % (2*stride) == 0) {\n",
        "                float lhs = data[idx];\n",
        "                float rhs = data[idx + stride];\n",
        "                data[idx] = lhs < rhs ? rhs : lhs;\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void Max_Interleaved_Addressing_Shared(float* data, int data_size){\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    __shared__ float sdata[MAX_CUDA_THREADS_PER_BLOCK];\n",
        "    if (idx < data_size){\n",
        "\n",
        "        /*copy to shared memory*/\n",
        "        sdata[threadIdx.x] = data[idx];\n",
        "        __syncthreads();\n",
        "\n",
        "        for(int stride=1; stride < blockDim.x; stride *= 2) {\n",
        "            if (threadIdx.x % (2*stride) == 0) {\n",
        "                float lhs = sdata[threadIdx.x];\n",
        "                float rhs = sdata[threadIdx.x + stride];\n",
        "                sdata[threadIdx.x] = lhs < rhs ? rhs : lhs;\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "    if (idx == 0) data[0] = sdata[0];\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void Max_Sequential_Addressing_Shared(float* data, int data_size){\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    __shared__ float sdata[MAX_CUDA_THREADS_PER_BLOCK];\n",
        "    if (idx < data_size){\n",
        "\n",
        "        /*copy to shared memory*/\n",
        "        sdata[threadIdx.x] = data[idx];\n",
        "        __syncthreads();\n",
        "\n",
        "        for(int stride=blockDim.x/2; stride > 0; stride /= 2) {\n",
        "            if (threadIdx.x < stride) {\n",
        "                float lhs = sdata[threadIdx.x];\n",
        "                float rhs = sdata[threadIdx.x + stride];\n",
        "                sdata[threadIdx.x] = lhs < rhs ? rhs : lhs;\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "    if (idx == 0) data[0] = sdata[0];\n",
        "}\n",
        "\n",
        "/*Algorithm Information. Includes pointers to different kernels, so they can be executed dynamically*/\n",
        "const int Algorithm_Count = 3;\n",
        "typedef void (*Kernel)(float *, int);\n",
        "const char* Algorithm_Name[Algorithm_Count]= {\"Max_Interleaved_Addressing_Global\", \"Max_Interleaved_Addressing_Shared\", \"Max_Sequential_Addressing_Shared\"};\n",
        "const Kernel Algorithm[Algorithm_Count]    = { Max_Interleaved_Addressing_Global,   Max_Interleaved_Addressing_Shared,   Max_Sequential_Addressing_Shared};\n",
        "\n",
        "Result calculateMaxValue(DataSet data, Kernel algorithm){\n",
        "    float* device_data;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);    \n",
        "\n",
        "    gpuErrchk(cudaMalloc((void **)&device_data,  sizeof(float)*data.size));\n",
        "    gpuErrchk(cudaMemcpy(device_data, data.values, sizeof(float)*data.size, cudaMemcpyHostToDevice));\n",
        "\n",
        "\n",
        "    int threads_needed = data.size;\n",
        "    cudaEventRecord(start);\n",
        "    algorithm<<< threads_needed/ startup.threads_per_block + 1, startup.threads_per_block>>>(device_data, data.size);\n",
        "    cudaEventRecord(stop);\n",
        "    gpuErrchk(cudaEventSynchronize(stop));\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "    float max_value;\n",
        "    gpuErrchk(cudaMemcpy(&max_value, device_data, sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    gpuErrchk(cudaFree(device_data));\n",
        "\n",
        "    Result r = {max_value, milliseconds};\n",
        "    return r;\n",
        "}\n",
        "\n",
        "Result calculateMaxValue(DataSet data){\n",
        "    return calculateMaxValue(data, Algorithm[Algorithm_Count - 1]);\n",
        "}\n",
        "\n",
        "void printDataSet(DataSet data){\n",
        "    for (int i = 0; i < data.size; i++)\n",
        "        printf(\"%.6g, \", data.values[i]);\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "void benchmarkCSV(){\n",
        "    /*Print Headers*/\n",
        "    printf(\"Elements, \");\n",
        "    for (int algoID = 0; algoID < Algorithm_Count; algoID++)\n",
        "        printf(\"%s, \", Algorithm_Name[algoID]);\n",
        "    printf(\"\\n\");\n",
        "    /*Benchmark*/\n",
        "    for (int dataSize = 2; dataSize < INT_MAX; dataSize*=2){\n",
        "        DataSet random = generateRandomDataSet(dataSize);\n",
        "        printf(\"%d, \", dataSize);\n",
        "        for (int algoID = 0; algoID < Algorithm_Count; algoID++) {\n",
        "            Result r = calculateMaxValue(random, Algorithm[algoID]);\n",
        "            printf(\"%g, \", r.KernelExecutionTime);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "        free(random.values);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "    srand(time(nullptr));\n",
        "    benchmarkCSV();\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 1073741824 bytes == 0x55e576c82000 @  0x7f824edfb1e7 0x55e574bbb167 0x55e574bbb53f 0x55e574bbb618 0x7f824de2cbf7 0x55e574bbb02a\n",
            "tcmalloc: large alloc 2147483648 bytes == 0x55e576c82000 @  0x7f824edfb1e7 0x55e574bbb167 0x55e574bbb53f 0x55e574bbb618 0x7f824de2cbf7 0x55e574bbb02a\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55e576c82000 @  0x7f824edfb1e7 0x55e574bbb167 0x55e574bbb53f 0x55e574bbb618 0x7f824de2cbf7 0x55e574bbb02a\n",
            "tcmalloc: large alloc 18446744065119617024 bytes == (nil) @  0x7f824edfb1e7 0x55e574bbb167 0x55e574bbb53f 0x55e574bbb618 0x7f824de2cbf7 0x55e574bbb02a\n",
            "GPUassert: out of memory /tmp/tmpr3mxsagt/43c50341-76f8-41e9-ae40-08b081c94125.cu 112\n",
            "Elements, Max_Interleaved_Addressing_Global, Max_Interleaved_Addressing_Shared, Max_Sequential_Addressing_Shared, \n",
            "2, 0.12048, 0.021696, 0.021312, \n",
            "4, 0.016416, 0.02032, 0.01776, \n",
            "8, 0.01552, 0.020896, 0.017536, \n",
            "16, 0.015168, 0.020864, 0.017664, \n",
            "32, 0.01664, 0.02064, 0.017504, \n",
            "64, 0.01872, 0.033184, 0.017952, \n",
            "128, 0.018912, 0.019584, 0.016608, \n",
            "256, 0.020736, 0.020896, 0.01728, \n",
            "512, 0.021536, 0.018528, 0.016288, \n",
            "1024, 0.023968, 0.021024, 0.01712, \n",
            "2048, 0.025088, 0.021056, 0.016096, \n",
            "4096, 0.025728, 0.02016, 0.01552, \n",
            "8192, 0.030368, 0.02384, 0.018528, \n",
            "16384, 0.03456, 0.025824, 0.019936, \n",
            "32768, 0.0584, 0.039904, 0.031328, \n",
            "65536, 0.084384, 0.052544, 0.03824, \n",
            "131072, 0.148832, 0.093824, 0.059936, \n",
            "262144, 0.289792, 0.153024, 0.105792, \n",
            "524288, 0.58, 0.292288, 0.186944, \n",
            "1048576, 1.17267, 0.561568, 0.349696, \n",
            "2097152, 2.39578, 1.09888, 0.682304, \n",
            "4194304, 4.92813, 2.17402, 1.3423, \n",
            "8388608, 10.1915, 4.33578, 2.65965, \n",
            "16777216, 21.0916, 8.63789, 5.296, \n",
            "33554432, 43.5481, 17.263, 9.47661, \n",
            "67108864, 80.2564, 30.2042, 17.0793, \n",
            "134217728, 172.597, 55.8017, 30.857, \n",
            "268435456, 248.47, 89.18, 54.5864, \n",
            "536870912, 508.416, 179.065, 109.313, \n",
            "1073741824, 1045.29, 358.265, 218.59, \n",
            "-2147483648, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaArbFcfA-9O"
      },
      "source": [
        "## My version\n",
        "\n",
        "**Output file #1:**\n",
        "* The number of tests(5 for: 1M, 5M, 10M, 15M, 20M)\n",
        "* Each test's size and runtime\n",
        "\n",
        "**Output file #2:**\n",
        "* The number of tests\n",
        "* Each test's generated numbers, result of max/min function, runtime and size\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcTl0rW5BD-1",
        "outputId": "cd1e76e8-0687-4d05-a2d5-fdc8aef79c37"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <time.h>\n",
        "\n",
        "\n",
        "#define MAX_CUDA_THREADS_PER_BLOCK 1024\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true){\n",
        "   if (code != cudaSuccess) {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "\n",
        "struct Startup{\n",
        "    int random_range = INT_MAX;\n",
        "    int threads_per_block = MAX_CUDA_THREADS_PER_BLOCK;\n",
        "} startup;\n",
        "\n",
        "struct DataSet{\n",
        "    int* values;\n",
        "    int  size;\n",
        "};\n",
        "\n",
        "struct Result{\n",
        "    int MaxValue;\n",
        "    float KernelExecutionTime;\n",
        "};\n",
        "\n",
        "DataSet generateRandomDataSet(int size){\n",
        "    DataSet data;\n",
        "    data.size = size;\n",
        "    data.values = (int*)malloc(sizeof(int)*data.size);\n",
        "\n",
        "    for (int i = 0; i < data.size; i++)\n",
        "        data.values[i] = (int)(rand()%startup.random_range);\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "__global__ void Max_Sequential_Addressing_Shared(int* data, int data_size){\n",
        "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    __shared__ int sdata[MAX_CUDA_THREADS_PER_BLOCK];\n",
        "    if (idx < data_size){\n",
        "\n",
        "        /*copy to shared memory*/\n",
        "        sdata[threadIdx.x] = data[idx];\n",
        "        __syncthreads();\n",
        "\n",
        "        for(int stride=blockDim.x/2; stride > 0; stride /= 2) {\n",
        "            if (threadIdx.x < stride) {\n",
        "                int lhs = sdata[threadIdx.x];\n",
        "                int rhs = sdata[threadIdx.x + stride];\n",
        "                sdata[threadIdx.x] = lhs < rhs ? rhs : lhs;\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "    if (idx == 0) data[0] = sdata[0];\n",
        "}\n",
        "\n",
        "\n",
        "Result calculateMaxValue(DataSet data){\n",
        "    int* device_data;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);    \n",
        "\n",
        "    gpuErrchk(cudaMalloc((void **)&device_data,  sizeof(int)*data.size));\n",
        "    gpuErrchk(cudaMemcpy(device_data, data.values, sizeof(int)*data.size, cudaMemcpyHostToDevice));\n",
        "\n",
        "\n",
        "    int threads_needed = data.size;\n",
        "    cudaEventRecord(start);\n",
        "    Max_Sequential_Addressing_Shared<<< threads_needed/ startup.threads_per_block + 1, startup.threads_per_block>>>(device_data, data.size);\n",
        "    cudaEventRecord(stop);\n",
        "    gpuErrchk(cudaEventSynchronize(stop));\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "    int max_value;\n",
        "    gpuErrchk(cudaMemcpy(&max_value, device_data, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    gpuErrchk(cudaFree(device_data));\n",
        "\n",
        "    Result r = {max_value, milliseconds}; // this might cause error\n",
        "    return r;\n",
        "}\n",
        "\n",
        "\n",
        "void printDataSet(DataSet data){\n",
        "    for (int i = 0; i < data.size; i++)\n",
        "        printf(\"%d, \", data.values[i]);\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "\n",
        "void benchmarkCSV(){\n",
        "    /*Benchmark*/\n",
        "    FILE *out_file = fopen(\"Results\", \"w\");\n",
        "\n",
        "    int size[] = {1, 5, 10, 15, 20};\n",
        "    for (int i = 0; i<5; i++){\n",
        "        int dataSize = size[i]*1000000;\n",
        "        DataSet random = generateRandomDataSet(dataSize);\n",
        "        Result r = calculateMaxValue(random);\n",
        "\n",
        "        fprintf(out_file, \"Data size: %d\\n\", dataSize);\n",
        "        //fprintf(out_file, \"Maximum value: %d\\n\", r.MaxValue);\n",
        "        fprintf(out_file, \"Execution time: %f\\n----\\n\", r.KernelExecutionTime);\n",
        "\n",
        "        printf(\"%d, \", dataSize);\n",
        "        printf(\"%g, \", r.KernelExecutionTime);\n",
        "        printf(\"\\n\");\n",
        "        free(random.values);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "    srand(time(nullptr));\n",
        "    benchmarkCSV();\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000, 0.486816, \n",
            "5000000, 1.78016, \n",
            "10000000, 3.53635, \n",
            "15000000, 5.29286, \n",
            "20000000, 7.06013, \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yofGq1AmmyEO",
        "outputId": "5230e98b-cad5-4293-ffa7-8a8d1370021e"
      },
      "source": [
        "! ls\n",
        "! nvcc --version\n",
        "! nvcc -o max-reduce max-reduce.cu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb  Results\tsrc\n",
            "max-reduce.cu\t\t\t\t\t   sample_data\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Wed_Apr_11_23:16:29_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHbFaMU_nKPP",
        "outputId": "e4571a05-d091-43d1-8cc1-146fc56da64d"
      },
      "source": [
        "! ./max-reduce"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000, 0.488224, \n",
            "5000000, 1.7848, \n",
            "10000000, 3.53549, \n",
            "15000000, 5.30483, \n",
            "20000000, 7.05405, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj5RoXH9nX_u",
        "outputId": "63b06954-2da8-4805-d853-ad3204b3a96b"
      },
      "source": [
        "! nvprof ./max-reduce"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==18669== NVPROF is profiling process 18669, command: ./max-reduce\n",
            "==18669== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "1000000, 0.56912, \n",
            "5000000, 1.84294, \n",
            "10000000, 3.67098, \n",
            "15000000, 5.49165, \n",
            "20000000, 7.31232, \n",
            "==18669== Profiling application: ./max-reduce\n",
            "==18669== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   60.23%  28.219ms         5  5.6439ms  554.87us  10.934ms  [CUDA memcpy HtoD]\n",
            "                   39.74%  18.619ms         5  3.7239ms  370.40us  7.2974ms  Max_Sequential_Addressing_Shared(int*, int)\n",
            "                    0.03%  16.288us         5  3.2570us  2.8800us  3.5520us  [CUDA memcpy DtoH]\n",
            "      API calls:   83.22%  257.23ms        10  25.723ms     764ns  257.16ms  cudaEventCreate\n",
            "                    9.30%  28.733ms        10  2.8733ms  28.879us  11.024ms  cudaMemcpy\n",
            "                    6.15%  18.997ms         5  3.7994ms  360.28us  7.3857ms  cudaEventSynchronize\n",
            "                    0.55%  1.6850ms         5  337.00us  244.47us  395.79us  cudaMalloc\n",
            "                    0.35%  1.0874ms         5  217.48us  130.45us  327.64us  cudaFree\n",
            "                    0.16%  500.63us         5  100.13us  34.731us  331.66us  cudaLaunchKernel\n",
            "                    0.15%  469.53us         1  469.53us  469.53us  469.53us  cuDeviceTotalMem\n",
            "                    0.07%  227.95us        96  2.3740us     115ns  100.81us  cuDeviceGetAttribute\n",
            "                    0.02%  75.772us        10  7.5770us  2.6770us  19.151us  cudaEventRecord\n",
            "                    0.02%  55.251us         1  55.251us  55.251us  55.251us  cuDeviceGetName\n",
            "                    0.01%  20.331us         5  4.0660us  2.2910us  6.0530us  cudaEventElapsedTime\n",
            "                    0.01%  17.969us         1  17.969us  17.969us  17.969us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.1700us         3  1.0560us     323ns  1.5600us  cuDeviceGetCount\n",
            "                    0.00%  1.7880us         2     894ns     279ns  1.5090us  cuDeviceGet\n"
          ]
        }
      ]
    }
  ]
}